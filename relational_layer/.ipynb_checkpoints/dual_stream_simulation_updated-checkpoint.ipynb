{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854cf646",
   "metadata": {},
   "source": [
    "# Dual Stream Relational Simulation\n",
    "This notebook simulates relational token memory and saves results to JSON with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f27dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user and AI streams\n",
    "user_stream = [\n",
    "    \"Let's run all of today's content through the simulation.\",\n",
    "    \"Should we build a new repo or just add to the existing one?\",\n",
    "    \"I'm wondering what you think about capturing both sides of the conversation.\",\n",
    "    \"Feels like an evolution of what we started with.\"\n",
    "]\n",
    "\n",
    "ai_stream = [\n",
    "    \"Yes, we can simulate relational salience over time.\",\n",
    "    \"Let's extend the existing token memory layer with relational weighting.\",\n",
    "    \"Capturing both sides gives us insight into mirrored salience and drift.\",\n",
    "    \"Agreed. This belongs in the same repo with an upgraded README.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5dde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple weighting function\n",
    "def tokenize_and_weight(stream, label, boost=1.0):\n",
    "    weights = defaultdict(float)\n",
    "    for i, sentence in enumerate(stream):\n",
    "        tokens = sentence.lower().split()\n",
    "        for j, token in enumerate(tokens):\n",
    "            weights[f\"{label}:{token}\"] += 1.0 / (j + 1) * boost\n",
    "    return weights\n",
    "\n",
    "user_weights = tokenize_and_weight(user_stream, \"USER\", boost=1.2)\n",
    "ai_weights = tokenize_and_weight(ai_stream, \"AI\", boost=1.0)\n",
    "\n",
    "# Combine weights\n",
    "relational_weights = {token: (user_weights.get(token, 0), ai_weights.get(token, 0))\n",
    "                      for token in set(user_weights) | set(ai_weights)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbc54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.DataFrame([\n",
    "    {\"token\": token, \"user_weight\": uw, \"ai_weight\": aw}\n",
    "    for token, (uw, aw) in relational_weights.items()\n",
    "])\n",
    "df_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ae0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add timestamp\n",
    "df_weights[\"timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "# Save to relational_weights.json\n",
    "json_file = \"relational_weights.json\"\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        existing_data = json.load(f)\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "# Append new session data\n",
    "existing_data.extend(df_weights.to_dict(orient=\"records\"))\n",
    "\n",
    "with open(json_file, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved {len(df_weights)} rows to {json_file}. Total records: {len(existing_data)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ac71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top weighted tokens\n",
    "top_tokens = df_weights.sort_values(by=[\"user_weight\", \"ai_weight\"], ascending=False).head(10)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(top_tokens[\"token\"], top_tokens[\"user_weight\"], label=\"User\")\n",
    "plt.barh(top_tokens[\"token\"], top_tokens[\"ai_weight\"], left=top_tokens[\"user_weight\"], label=\"AI\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.title(\"Top Relational Tokens (User + AI)\")\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}